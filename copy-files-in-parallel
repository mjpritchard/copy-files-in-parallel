#!/usr/bin/env python
import argparse
import os
import subprocess
import sys
import uuid

parser = argparse.ArgumentParser(description='Copy files in parallel.')
parser.add_argument('source', help='source path')
parser.add_argument('destination', help='destination path')
parser.add_argument('-j', default=8, type=int, help='number of threads to use')
parser.add_argument('-c', default=1000, type=int, help='number of files in one chunk')

parser.add_argument('--resume', default=False, action='store_true', help='resume previous job (skip finding files)')
parser.add_argument('--dry', action='store_true', help='perform a trial run with no changes made')
parser.add_argument('--lfs', action='store_true', help='use lustre utilities')
parser.add_argument('--arcfour', action='store_true', help='use arcfour as compression algorithm')
parser.add_argument('--tmp', default='/tmp/', help='path to store temporary data')

args = parser.parse_args()

# check if GNU parallel is installed
if not any(os.access(os.path.join(path, 'parallel'), os.X_OK) for path in os.environ["PATH"].split(os.pathsep)):
    raise Exception('GNU parallel is not installed.')

# remove trailing slashes
source = args.source.rstrip('/')
destination = args.destination.rstrip('/')

# prepare find exe
if args.lfs:
    find = 'lfs find'
else:
    find = 'find'

if args.resume:
    # prepare files/directories
    tmp_dir = os.path.join(args.resume)
    if not os.path.isdir(tmp_dir):
        sys.exit('Error: Can not find jobs tmp directory.')

    files_file = os.path.join(tmp_dir, 'files')
    jobs_file = os.path.join(tmp_dir, 'jobs')

else:
    # prepare files/directories
    tmp_dir = os.path.join(args.tmp, 'copy-files-in-parallel', str(uuid.uuid4()))
    os.makedirs(tmp_dir)

    files_file = os.path.join(tmp_dir, 'files')
    jobs_file = os.path.join(tmp_dir, 'jobs')

    # find files
    if ':' in source:
        source_um, source_path = source.split(':')
        subprocess.run(f'ssh {source_um} "cd {source_path}; {find} . -type f" > {files_file}', shell=True, check=True)
    else:
        subprocess.run(f'{find} . -type f > {files_file}', shell=True, check=True, cwd=source)

    # split the files file in chunks of 1000
    chunk_file = os.path.join(tmp_dir, 'chunk')
    subprocess.run(f'split -l {args.c} -d -a 8 {files_file} {chunk_file}', shell=True, check=True)

    # create a file containing all the copy jobs to perform
    with open(jobs_file, 'w') as jobs_file_handler:
        for filename in sorted(os.listdir(tmp_dir)):
            if filename.startswith('chunk'):
                chunk_file = os.path.join(tmp_dir, filename)
                chunk_num = filename.split('chunk')[1]
                log_file = os.path.join(tmp_dir, 'log' + chunk_num)

                job = 'rsync -aH --no-l'
                if args.arcfour:
                    job += ' -e \'ssh -c arcfour\''
                job += f' --files-from={chunk_file} --log-file={log_file} {source} {destination}\n'

                jobs_file_handler.write(job)

try:
    n_files = subprocess.run(f'wc -l {files_file}', shell=True, check=True, capture_output=True, text=True).stdout.split()[0]
    n_jobs = subprocess.run(f'wc -l {jobs_file}', shell=True, check=True, capture_output=True, text=True).stdout.split()[0]

    print(f'Copying {n_files} files using {n_jobs} jobs!')
    print(f'tmp_dir is {tmp_dir}')
except AttributeError:
    pass

# run parallel with the jobs file
subprocess.run(f'cat {jobs_file} | parallel -P{args.j} --eta', shell=True, check=True)